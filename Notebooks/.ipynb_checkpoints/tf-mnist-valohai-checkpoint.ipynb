{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valohai Notebook Sample\n",
    "\n",
    "1. **Settings:**\n",
    "    * **Project:** Valohai project where the executions will be version controlled\n",
    "    * **Environment:** Environment type for the cloud executions (E.g. AWS p2.xlarge)\n",
    "    * **Docker Image:** Docker image that provides the required libraries (E.g. TensorFlow) `valohai/pypermill` is the default image, but for this sample use `drazend/mnist-notebook:latest`\n",
    "\n",
    "2. **Data:**\n",
    "    * Upload your data to a cloud storage. You can use app.valohai.com to do this (Go to *Project -> Data -> Upload*)\n",
    "    * Create a new cell for inputs. For each input create a variable with a link to the file in your cloud storage\n",
    "    * All your data will be download to `/valohai/inputs/<name-of-input-variable/`.\n",
    "        * `training_set_images = \"https://valohaidemo.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz\"` will get downloaded to `/valohai/inputs/training_set_images/train-images-idx3-ubyte.gz`\n",
    "    * Tag the cell as \"inputs\" (In this notebook UI *View->Cell Toolbar->Tags*)\n",
    "    * Read all your data from `/valohai/inputs/` and save all your data to `/valohai/outputs/`\n",
    "\n",
    "3. **Parameters:**\n",
    "    * Create a new cell for parameters.\n",
    "    * Place all your parameters in this cell.\n",
    "    * Tag the cell as \"parameters\" (In this notebook UI *View->Cell Toolbar->Tags*)\n",
    "    \n",
    "4. **Tracking key metrics:**\n",
    "    * Print out all key metrics as JSON (key/value)\n",
    "        * `print(json.dumps({\"step\": int(i), \"accuracy\": float(acc), \"loss\": float(ce)}))`\n",
    "    * If you'd like to save more complex plots and graphs, you can save them to `/valohai/ouputs/`\n",
    "        * plt.savefig(`/valohai/outputs/graph.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012879,
     "end_time": "2019-09-24T11:20:46.960131",
     "exception": false,
     "start_time": "2019-09-24T11:20:46.947252",
     "status": "completed"
    },
    "tags": [
     "inputs"
    ]
   },
   "outputs": [],
   "source": [
    "training_set_images = \"https://valohaidemo.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz\"\n",
    "training_set_labels = \"https://valohaidemo.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz\"\n",
    "test_set_images = \"https://valohaidemo.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz\"\n",
    "test_set_labels = \"https://valohaidemo.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009771,
     "end_time": "2019-09-24T11:20:46.975537",
     "exception": false,
     "start_time": "2019-09-24T11:20:46.965766",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "max_steps = 300\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.804253,
     "end_time": "2019-09-24T11:20:48.784686",
     "exception": false,
     "start_time": "2019-09-24T11:20:46.980433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "    \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '':\n",
    "    device_name = \"None\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.010855,
     "end_time": "2019-09-24T11:20:48.800752",
     "exception": false,
     "start_time": "2019-09-24T11:20:48.789897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Using TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.852726,
     "end_time": "2019-09-24T11:20:49.658621",
     "exception": false,
     "start_time": "2019-09-24T11:20:48.805895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "inputs_dir = os.getenv('VH_INPUTS_DIR', 'data')\n",
    "if inputs_dir != 'data':\n",
    "    data_dir = '/tmp/data'\n",
    "    os.mkdir(data_dir)\n",
    "    for file in glob.glob(inputs_dir + \"/training_set_images/*.*\"):\n",
    "        shutil.copy(file, data_dir)\n",
    "    for file in glob.glob(inputs_dir + \"/training_set_labels/*.*\"):\n",
    "        shutil.copy(file, data_dir)\n",
    "    for file in glob.glob(inputs_dir + \"/test_set_images/*.*\"):\n",
    "        shutil.copy(file, data_dir)\n",
    "    for file in glob.glob(inputs_dir + \"/test_set_labels/*.*\"):\n",
    "        shutil.copy(file, data_dir)\n",
    "    inputs_dir = data_dir\n",
    "        \n",
    "mnist = mnist_data.read_data_sets(inputs_dir,\n",
    "                                  one_hot=True,\n",
    "                                  reshape=False,\n",
    "                                  validation_size=0)\n",
    "\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.211387,
     "end_time": "2019-09-24T11:20:49.875948",
     "exception": false,
     "start_time": "2019-09-24T11:20:49.664561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholders for input images and correct labels:\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Variables for weights of two hidden layers:\n",
    "L1, L2 = 100, 200\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L1], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L1]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([L2]))\n",
    "W3 = tf.Variable(tf.truncated_normal([L2, 10], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# The MLP model:\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Ylogits = tf.matmul(Y2, W3) + B3\n",
    "Y = tf.nn.softmax(Ylogits, name='output')\n",
    "\n",
    "# Cross-entropy loss function:\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# Prediction accuracy:\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Learning rate and the used optimizer:\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.047974,
     "end_time": "2019-09-24T11:20:49.929980",
     "exception": false,
     "start_time": "2019-09-24T11:20:49.882006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.58927,
     "end_time": "2019-09-24T11:20:59.525365",
     "exception": false,
     "start_time": "2019-09-24T11:20:49.936095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_v,  ce_v = [], []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    batch_X, batch_Y = mnist.train.next_batch(batch_size) # minibatch\n",
    "    acc, ce, _ = sess.run([accuracy, cross_entropy, train_step], \n",
    "                          {X: batch_X, Y_: batch_Y})\n",
    "    acc_v.append(acc); ce_v.append(ce)\n",
    "    print(json.dumps({\"step\": int(i), \"accuracy\": float(acc), \"loss\": float(ce)}))\n",
    "    time.sleep(0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.379255,
     "end_time": "2019-09-24T11:20:59.921118",
     "exception": false,
     "start_time": "2019-09-24T11:20:59.541863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(max_steps), ce_v)\n",
    "plt.title('cross-entropy loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(max_steps), acc_v)\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037018,
     "end_time": "2019-09-24T11:20:59.975241",
     "exception": false,
     "start_time": "2019-09-24T11:20:59.938223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs_dir = os.getenv('VH_OUTPUTS_DIR', './outputs')\n",
    "if outputs_dir == './outputs':\n",
    "    if os.path.exists(outputs_dir):\n",
    "        shutil.rmtree(outputs_dir)\n",
    "    os.makedirs(outputs_dir)\n",
    "pbpath = os.path.join(outputs_dir, 'model.pb')\n",
    "output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess=sess,\n",
    "    input_graph_def=sess.graph.as_graph_def(),\n",
    "    output_node_names=['output'],\n",
    ")\n",
    "with tf.gfile.FastGFile(pbpath, 'wb') as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "    print (\"Saved %s\" % pbpath)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "duration": 14.408102,
   "end_time": "2019-09-24T11:21:00.699714",
   "environment_variables": {},
   "exception": null,
   "input_path": "/valohai/repository/tf-mnist-valohai.ipynb",
   "output_path": "/valohai/outputs/tf-mnist-valohai.ipynb",
   "parameters": {
    "batch_size": 100,
    "learning_rate": 0.001,
    "max_steps": 300
   },
   "start_time": "2019-09-24T11:20:46.291612",
   "version": "0.19.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
